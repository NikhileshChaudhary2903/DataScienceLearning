{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data',header=None,sep='\\s+')\n",
    "df.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']]\n",
    "\n",
    "Y=df['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.19251538838354\n",
      "[-1.14428903e-01  5.71299780e-02  3.83002824e-02  2.42854641e+00\n",
      " -2.12326236e+01  2.87723416e+00  6.91118094e-03 -1.47158266e+00\n",
      "  3.05784197e-01 -1.06750361e-02 -9.96138270e-01  6.27746234e-03\n",
      " -5.57414427e-01]\n"
     ]
    }
   ],
   "source": [
    "# print the intercept and coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CRIM', -0.11442890306628387),\n",
       " ('ZN', 0.05712997804798477),\n",
       " ('INDUS', 0.03830028239368708),\n",
       " ('CHAS', 2.4285464082017785),\n",
       " ('NOX', -21.23262356052882),\n",
       " ('RM', 2.877234156006178),\n",
       " ('AGE', 0.006911180936376129),\n",
       " ('DIS', -1.4715826611839433),\n",
       " ('RAD', 0.30578419712713745),\n",
       " ('TAX', -0.010675036077892434),\n",
       " ('PTRATIO', -0.996138269550698),\n",
       " ('B', 0.006277462341292743),\n",
       " ('LSTAT', -0.5574144267109944)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair the feature names with the coefficients\n",
    "feature_cols=X.columns.tolist()\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.679504823808729\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "                               \n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods=['Linear_Regression']\n",
    "vals=[]\n",
    "vals.append(np.sqrt(metrics.mean_squared_error(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.210244399890165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=324)\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth=20)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# compute the RMSE of our predictions\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods.append('Decision_Tree_Regressor')\n",
    "vals.append(np.sqrt(metrics.mean_squared_error(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   891.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Nov 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:06:31</td>     <th>  Log-Likelihood:    </th> <td> -1523.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   493</td>      <th>  BIC:               </th> <td>   3128.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>   -0.0929</td> <td>    0.034</td> <td>   -2.699</td> <td> 0.007</td> <td>   -0.161</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>    0.0487</td> <td>    0.014</td> <td>    3.382</td> <td> 0.001</td> <td>    0.020</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>   -0.0041</td> <td>    0.064</td> <td>   -0.063</td> <td> 0.950</td> <td>   -0.131</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>    2.8540</td> <td>    0.904</td> <td>    3.157</td> <td> 0.002</td> <td>    1.078</td> <td>    4.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>   -2.8684</td> <td>    3.359</td> <td>   -0.854</td> <td> 0.394</td> <td>   -9.468</td> <td>    3.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>    5.9281</td> <td>    0.309</td> <td>   19.178</td> <td> 0.000</td> <td>    5.321</td> <td>    6.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>   -0.0073</td> <td>    0.014</td> <td>   -0.526</td> <td> 0.599</td> <td>   -0.034</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>   -0.9685</td> <td>    0.196</td> <td>   -4.951</td> <td> 0.000</td> <td>   -1.353</td> <td>   -0.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>    0.1712</td> <td>    0.067</td> <td>    2.564</td> <td> 0.011</td> <td>    0.040</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>   -0.0094</td> <td>    0.004</td> <td>   -2.395</td> <td> 0.017</td> <td>   -0.017</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>   -0.3922</td> <td>    0.110</td> <td>   -3.570</td> <td> 0.000</td> <td>   -0.608</td> <td>   -0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>    0.0149</td> <td>    0.003</td> <td>    5.528</td> <td> 0.000</td> <td>    0.010</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>   -0.4163</td> <td>    0.051</td> <td>   -8.197</td> <td> 0.000</td> <td>   -0.516</td> <td>   -0.317</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>204.082</td> <th>  Durbin-Watson:     </th> <td>   0.999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1374.225</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.609</td>  <th>  Prob(JB):          </th> <td>3.90e-299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>10.404</td>  <th>  Cond. No.          </th> <td>8.50e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.5e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.959\n",
       "Model:                            OLS   Adj. R-squared:                  0.958\n",
       "Method:                 Least Squares   F-statistic:                     891.3\n",
       "Date:                Thu, 22 Nov 2018   Prob (F-statistic):               0.00\n",
       "Time:                        22:06:31   Log-Likelihood:                -1523.8\n",
       "No. Observations:                 506   AIC:                             3074.\n",
       "Df Residuals:                     493   BIC:                             3128.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "CRIM          -0.0929      0.034     -2.699      0.007      -0.161      -0.025\n",
       "ZN             0.0487      0.014      3.382      0.001       0.020       0.077\n",
       "INDUS         -0.0041      0.064     -0.063      0.950      -0.131       0.123\n",
       "CHAS           2.8540      0.904      3.157      0.002       1.078       4.630\n",
       "NOX           -2.8684      3.359     -0.854      0.394      -9.468       3.731\n",
       "RM             5.9281      0.309     19.178      0.000       5.321       6.535\n",
       "AGE           -0.0073      0.014     -0.526      0.599      -0.034       0.020\n",
       "DIS           -0.9685      0.196     -4.951      0.000      -1.353      -0.584\n",
       "RAD            0.1712      0.067      2.564      0.011       0.040       0.302\n",
       "TAX           -0.0094      0.004     -2.395      0.017      -0.017      -0.002\n",
       "PTRATIO       -0.3922      0.110     -3.570      0.000      -0.608      -0.176\n",
       "B              0.0149      0.003      5.528      0.000       0.010       0.020\n",
       "LSTAT         -0.4163      0.051     -8.197      0.000      -0.516      -0.317\n",
       "==============================================================================\n",
       "Omnibus:                      204.082   Durbin-Watson:                   0.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1374.225\n",
       "Skew:                           1.609   Prob(JB):                    3.90e-299\n",
       "Kurtosis:                      10.404   Cond. No.                     8.50e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.5e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model= 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X,Y)\n",
    "\n",
    "predictions = lm.predict(X)\n",
    "print('Accuracy of model=',lm.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train: 0.716, test : 0.789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## R2 score Evaluation\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_test_pred=model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"r2 train: %.3f, test : %.3f\" %(r2_score(y_train,y_train_pred),r2_score(y_test,y_test_pred) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X.values, Y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 5.8265 (1.7785)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 5.8257 (1.7787)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge score: 10.1189 (6.7615)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 4.3680 (1.4266)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 4.4279 (1.2353)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.32439944962841\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=1)\n",
    "\n",
    "forest=RandomForestRegressor(n_estimators=1000,criterion='mse',random_state=1,n_jobs=-1)\n",
    "forest.fit(X_train,y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# compute the RMSE of our predictions\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train: 0.979, test : 0.878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## R2 score Evaluation\n",
    "y_train_pred=forest.predict(X_train)\n",
    "y_test_pred=forest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"r2 train: %.3f, test : %.3f\" %(r2_score(y_train,y_train_pred),r2_score(y_test,y_test_pred) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods.append('Random Forest Regressor')\n",
    "vals.append(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Spectral6, brewer\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "ser_df=pd.DataFrame({'Model': mods, 'Accuracy_Value': vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(ser_df)\n",
    "\n",
    "p = figure(x_range=mods, plot_width=700, plot_height=500)\n",
    "color_map = factor_cmap(field_name='Model', palette=Spectral6, factors=mods)\n",
    "p.vbar(x='Model', top='Accuracy_Value', source=source, width=0.70, color=color_map)\n",
    "\n",
    "p.title.text ='Comparison of Models'\n",
    "p.xaxis.axis_label = 'Model Types'\n",
    "p.yaxis.axis_label = \"RMSE value of different models\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
